```python
# è‡ªåŠ¨è®¡ç®—cellçš„è®¡ç®—æ—¶é—´
%load_ext autotime

%matplotlib inline
%config InlineBackend.figure_format='svg' #çŸ¢é‡å›¾è®¾ç½®ï¼Œè®©ç»˜å›¾æ›´æ¸…æ™°
```

# 6-3,ä½¿ç”¨GPUè®­ç»ƒæ¨¡å‹


æ·±åº¦å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹å¸¸å¸¸éå¸¸è€—æ—¶ï¼Œä¸€ä¸ªæ¨¡å‹è®­ç»ƒå‡ ä¸ªå°æ—¶æ˜¯å®¶å¸¸ä¾¿é¥­ï¼Œè®­ç»ƒå‡ å¤©ä¹Ÿæ˜¯å¸¸æœ‰çš„äº‹æƒ…ï¼Œæœ‰æ—¶å€™ç”šè‡³è¦è®­ç»ƒå‡ åå¤©ã€‚

è®­ç»ƒè¿‡ç¨‹çš„è€—æ—¶ä¸»è¦æ¥è‡ªäºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ¥è‡ªæ•°æ®å‡†å¤‡ï¼Œå¦ä¸€éƒ¨åˆ†æ¥è‡ªå‚æ•°è¿­ä»£ã€‚

å½“æ•°æ®å‡†å¤‡è¿‡ç¨‹è¿˜æ˜¯æ¨¡å‹è®­ç»ƒæ—¶é—´çš„ä¸»è¦ç“¶é¢ˆæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤šè¿›ç¨‹æ¥å‡†å¤‡æ•°æ®ã€‚

å½“å‚æ•°è¿­ä»£è¿‡ç¨‹æˆä¸ºè®­ç»ƒæ—¶é—´çš„ä¸»è¦ç“¶é¢ˆæ—¶ï¼Œæˆ‘ä»¬é€šå¸¸çš„æ–¹æ³•æ˜¯åº”ç”¨GPUæ¥è¿›è¡ŒåŠ é€Ÿã€‚

<!-- #region -->
Pytorchä¸­ä½¿ç”¨GPUåŠ é€Ÿæ¨¡å‹éå¸¸ç®€å•ï¼Œåªè¦å°†æ¨¡å‹å’Œæ•°æ®ç§»åŠ¨åˆ°GPUä¸Šã€‚æ ¸å¿ƒä»£ç åªæœ‰ä»¥ä¸‹å‡ è¡Œã€‚

```python
# å®šä¹‰æ¨¡å‹
... 

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device) # ç§»åŠ¨æ¨¡å‹åˆ°cuda

# è®­ç»ƒæ¨¡å‹
...

features = features.to(device) # ç§»åŠ¨æ•°æ®åˆ°cuda
labels = labels.to(device) # æˆ–è€…  labels = labels.cuda() if torch.cuda.is_available() else labels
...
```

å¦‚æœè¦ä½¿ç”¨å¤šä¸ªGPUè®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿéå¸¸ç®€å•ã€‚åªéœ€è¦åœ¨å°†æ¨¡å‹è®¾ç½®ä¸ºæ•°æ®å¹¶è¡Œé£æ ¼æ¨¡å‹ã€‚
åˆ™æ¨¡å‹ç§»åŠ¨åˆ°GPUä¸Šä¹‹åï¼Œä¼šåœ¨æ¯ä¸€ä¸ªGPUä¸Šæ‹·è´ä¸€ä¸ªå‰¯æœ¬ï¼Œå¹¶æŠŠæ•°æ®å¹³åˆ†åˆ°å„ä¸ªGPUä¸Šè¿›è¡Œè®­ç»ƒã€‚æ ¸å¿ƒä»£ç å¦‚ä¸‹ã€‚

```python
# å®šä¹‰æ¨¡å‹
... 

if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model) # åŒ…è£…ä¸ºå¹¶è¡Œé£æ ¼æ¨¡å‹

# è®­ç»ƒæ¨¡å‹
...
features = features.to(device) # ç§»åŠ¨æ•°æ®åˆ°cuda
labels = labels.to(device) # æˆ–è€… labels = labels.cuda() if torch.cuda.is_available() else labels
...
```
<!-- #endregion -->

**ä»¥ä¸‹æ˜¯ä¸€äº›å’ŒGPUæœ‰å…³çš„åŸºæœ¬æ“ä½œæ±‡æ€»** 


åœ¨Colabç¬”è®°æœ¬ä¸­ï¼šä¿®æ”¹->ç¬”è®°æœ¬è®¾ç½®->ç¡¬ä»¶åŠ é€Ÿå™¨ ä¸­é€‰æ‹© GPU

æ³¨ï¼šä»¥ä¸‹ä»£ç åªèƒ½åœ¨Colab ä¸Šæ‰èƒ½æ­£ç¡®æ‰§è¡Œã€‚

å¯ç‚¹å‡»å¦‚ä¸‹é“¾æ¥ï¼Œç›´æ¥åœ¨colabä¸­è¿è¡ŒèŒƒä¾‹ä»£ç ã€‚

ã€Štorchä½¿ç”¨gpuè®­ç»ƒæ¨¡å‹ã€‹

https://colab.research.google.com/drive/1FDmi44-U3TFRCt9MwGn4HIj2SaaWIjHu?usp=sharing

```python
import torch 
from torch import nn 
```

```python
# 1ï¼ŒæŸ¥çœ‹gpuä¿¡æ¯
if_cuda = torch.cuda.is_available()
print("if_cuda=",if_cuda)

gpu_count = torch.cuda.device_count()
print("gpu_count=",gpu_count)
```

```python
# 2ï¼Œå°†å¼ é‡åœ¨gpuå’Œcpué—´ç§»åŠ¨
tensor = torch.rand((100,100))
tensor_gpu = tensor.to("cuda:1") # æˆ–è€… tensor_gpu = tensor.cuda()
print(tensor_gpu.device)
print(tensor_gpu.is_cuda)

tensor_cpu = tensor_gpu.to("cpu") # æˆ–è€… tensor_cpu = tensor_gpu.cpu() 
print(tensor_cpu.device)
```

```python
# 3ï¼Œå°†æ¨¡å‹ä¸­çš„å…¨éƒ¨å¼ é‡ç§»åŠ¨åˆ°gpuä¸Š
net = nn.Linear(2,1)
print(next(net.parameters()).is_cuda)
net.to("cuda:0") # å°†æ¨¡å‹ä¸­çš„å…¨éƒ¨å‚æ•°å¼ é‡ä¾æ¬¡åˆ°GPUä¸Šï¼Œæ³¨æ„ï¼Œæ— éœ€é‡æ–°èµ‹å€¼ä¸º net = net.to("cuda:0")
print(next(net.parameters()).is_cuda)
print(next(net.parameters()).device)
```

```python
# 4ï¼Œåˆ›å»ºæ”¯æŒå¤šä¸ªgpuæ•°æ®å¹¶è¡Œçš„æ¨¡å‹
linear = nn.Linear(2,1)
print(next(linear.parameters()).device)

model = nn.DataParallel(linear)
print(model.device_ids)
print(next(model.module.parameters()).device) 

#æ³¨æ„ä¿å­˜å‚æ•°æ—¶è¦æŒ‡å®šä¿å­˜model.moduleçš„å‚æ•°
torch.save(model.module.state_dict(), "./data/model_parameter.pkl") 

linear = nn.Linear(2,1)
linear.load_state_dict(torch.load("./data/model_parameter.pkl")) 
```

```
cpu
[0]
cuda:0
```

```python
# 5ï¼Œæ¸…ç©ºcudaç¼“å­˜

# è¯¥æ–¹æ³•åœ¨cudaè¶…å†…å­˜æ—¶ååˆ†æœ‰ç”¨
torch.cuda.empty_cache()
```

### ä¸€ï¼ŒçŸ©é˜µä¹˜æ³•èŒƒä¾‹


ä¸‹é¢åˆ†åˆ«ä½¿ç”¨CPUå’ŒGPUä½œä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œå¹¶æ¯”è¾ƒå…¶è®¡ç®—æ•ˆç‡ã€‚

```python
import time
import torch 
from torch import nn
```

```python
# ä½¿ç”¨cpu
a = torch.rand((100000,200))
b = torch.rand((200,10000))
tic = time.time()
c = torch.matmul(a,b)
toc = time.time()

print(toc-tic)
print(a.device)
print(b.device)
```

```python
# ä½¿ç”¨gpu
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
a = torch.rand((100000,200),device = device) #å¯ä»¥æŒ‡å®šåœ¨GPUä¸Šåˆ›å»ºå¼ é‡
b = torch.rand((200,10000)) #ä¹Ÿå¯ä»¥åœ¨CPUä¸Šåˆ›å»ºå¼ é‡åç§»åŠ¨åˆ°GPUä¸Š
b = b.to(device) #æˆ–è€… b = b.cuda() if torch.cuda.is_available() else b 
tic = time.time()
c = torch.matmul(a,b)
toc = time.time()
print(toc-tic)
print(a.device)
print(b.device)
```

### äºŒï¼Œçº¿æ€§å›å½’èŒƒä¾‹



ä¸‹é¢å¯¹æ¯”ä½¿ç”¨CPUå’ŒGPUè®­ç»ƒä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹çš„æ•ˆç‡


**1ï¼Œä½¿ç”¨CPU**

```python
# å‡†å¤‡æ•°æ®
n = 1000000 #æ ·æœ¬æ•°é‡

X = 10*torch.rand([n,2])-5.0  #torch.randæ˜¯å‡åŒ€åˆ†å¸ƒ 
w0 = torch.tensor([[2.0,-3.0]])
b0 = torch.tensor([[10.0]])
Y = X@w0.t() + b0 + torch.normal( 0.0,2.0,size = [n,1])  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨
```

```python
# å®šä¹‰æ¨¡å‹
class LinearRegression(nn.Module): 
    def __init__(self):
        super().__init__()
        self.w = nn.Parameter(torch.randn_like(w0))
        self.b = nn.Parameter(torch.zeros_like(b0))
    #æ­£å‘ä¼ æ’­
    def forward(self,x): 
        return x@self.w.t() + self.b
        
linear = LinearRegression() 
```

```python
# è®­ç»ƒæ¨¡å‹
optimizer = torch.optim.Adam(linear.parameters(),lr = 0.1)
loss_func = nn.MSELoss()

def train(epoches):
    tic = time.time()
    for epoch in range(epoches):
        optimizer.zero_grad()
        Y_pred = linear(X) 
        loss = loss_func(Y_pred,Y)
        loss.backward() 
        optimizer.step()
        if epoch%50==0:
            print({"epoch":epoch,"loss":loss.item()})
    toc = time.time()
    print("time used:",toc-tic)

train(500)
```

**2ï¼Œä½¿ç”¨GPU**

```python
# å‡†å¤‡æ•°æ®
n = 1000000 #æ ·æœ¬æ•°é‡

X = 10*torch.rand([n,2])-5.0  #torch.randæ˜¯å‡åŒ€åˆ†å¸ƒ 
w0 = torch.tensor([[2.0,-3.0]])
b0 = torch.tensor([[10.0]])
Y = X@w0.t() + b0 + torch.normal( 0.0,2.0,size = [n,1])  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨

# ç§»åŠ¨åˆ°GPUä¸Š
print("torch.cuda.is_available() = ",torch.cuda.is_available())
X = X.cuda()
Y = Y.cuda()
print("X.device:",X.device)
print("Y.device:",Y.device)
```

```python
# å®šä¹‰æ¨¡å‹
class LinearRegression(nn.Module): 
    def __init__(self):
        super().__init__()
        self.w = nn.Parameter(torch.randn_like(w0))
        self.b = nn.Parameter(torch.zeros_like(b0))
    #æ­£å‘ä¼ æ’­
    def forward(self,x): 
        return x@self.w.t() + self.b
        
linear = LinearRegression() 

# ç§»åŠ¨æ¨¡å‹åˆ°GPUä¸Š
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
linear.to(device)

#æŸ¥çœ‹æ¨¡å‹æ˜¯å¦å·²ç»ç§»åŠ¨åˆ°GPUä¸Š
print("if on cuda:",next(linear.parameters()).is_cuda)
```

```python
# è®­ç»ƒæ¨¡å‹
optimizer = torch.optim.Adam(linear.parameters(),lr = 0.1)
loss_func = nn.MSELoss()

def train(epoches):
    tic = time.time()
    for epoch in range(epoches):
        optimizer.zero_grad()
        Y_pred = linear(X) 
        loss = loss_func(Y_pred,Y)
        loss.backward() 
        optimizer.step()
        if epoch%50==0:
            print({"epoch":epoch,"loss":loss.item()})
    toc = time.time()
    print("time used:",toc-tic)
    
train(500)
```

### ä¸‰ï¼Œtorchkeras.Modelä½¿ç”¨å•GPUèŒƒä¾‹


ä¸‹é¢æ¼”ç¤ºä½¿ç”¨torchkeras.Modelæ¥åº”ç”¨GPUè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚

å…¶å¯¹åº”çš„CPUè®­ç»ƒæ¨¡å‹ä»£ç å‚è§ã€Š6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•ã€‹

æœ¬ä¾‹ä»…éœ€è¦åœ¨å®ƒçš„åŸºç¡€ä¸Šå¢åŠ ä¸€è¡Œä»£ç ï¼Œåœ¨model.compileæ—¶æŒ‡å®š deviceå³å¯ã€‚



**1ï¼Œå‡†å¤‡æ•°æ®**

```python
!pip install -U torchkeras 
```

```python
import torch 
from torch import nn 

import torchvision 
from torchvision import transforms

import torchkeras 
```

```python
transform = transforms.Compose([transforms.ToTensor()])

ds_train = torchvision.datasets.MNIST(root="./data/minist/",train=True,download=True,transform=transform)
ds_valid = torchvision.datasets.MNIST(root="./data/minist/",train=False,download=True,transform=transform)

dl_train =  torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=4)
dl_valid =  torch.utils.data.DataLoader(ds_valid, batch_size=128, shuffle=False, num_workers=4)

print(len(ds_train))
print(len(ds_valid))
```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æŸ¥çœ‹éƒ¨åˆ†æ ·æœ¬
from matplotlib import pyplot as plt 

plt.figure(figsize=(8,8)) 
for i in range(9):
    img,label = ds_train[i]
    img = torch.squeeze(img)
    ax=plt.subplot(3,3,i+1)
    ax.imshow(img.numpy())
    ax.set_title("label = %d"%label)
    ax.set_xticks([])
    ax.set_yticks([]) 
plt.show()
```

**2ï¼Œå®šä¹‰æ¨¡å‹**

```python
class CnnModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,10)]
        )
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)
        return x

net = CnnModel()
model = torchkeras.Model(net)
model.summary(input_shape=(1,32,32))
```

**3ï¼Œè®­ç»ƒæ¨¡å‹**

```python
from sklearn.metrics import accuracy_score

def accuracy(y_pred,y_true):
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data
    return accuracy_score(y_true.cpu().numpy(),y_pred_cls.cpu().numpy()) 
    # æ³¨æ„æ­¤å¤„è¦å°†æ•°æ®å…ˆç§»åŠ¨åˆ°cpuä¸Šï¼Œç„¶åæ‰èƒ½è½¬æ¢æˆnumpyæ•°ç»„

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model.compile(loss_func = nn.CrossEntropyLoss(),
             optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),
             metrics_dict={"accuracy":accuracy},device = device) # æ³¨æ„æ­¤å¤„compileæ—¶æŒ‡å®šäº†device

dfhistory = model.fit(3,dl_train = dl_train, dl_val=dl_valid, log_step_freq=100) 
```

**4ï¼Œè¯„ä¼°æ¨¡å‹**

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory,"loss")
```

```python
plot_metric(dfhistory,"accuracy")
```

```python
model.evaluate(dl_valid)
```

```
{'val_accuracy': 0.967068829113924, 'val_loss': 0.11601964030650598}
```


**5ï¼Œä½¿ç”¨æ¨¡å‹**

```python
model.predict(dl_valid)[0:10]
```

**6ï¼Œä¿å­˜æ¨¡å‹**

```python
# save the model parameters
torch.save(model.state_dict(), "model_parameter.pkl")

model_clone = torchkeras.Model(CnnModel())
model_clone.load_state_dict(torch.load("model_parameter.pkl"))

model_clone.compile(loss_func = nn.CrossEntropyLoss(),
             optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),
             metrics_dict={"accuracy":accuracy},device = device) # æ³¨æ„æ­¤å¤„compileæ—¶æŒ‡å®šäº†device

model_clone.evaluate(dl_valid)
```

### å››ï¼Œtorchkeras.Modelä½¿ç”¨å¤šGPUèŒƒä¾‹


æ³¨ï¼šä»¥ä¸‹èŒƒä¾‹éœ€è¦åœ¨æœ‰å¤šä¸ªGPUçš„æœºå™¨ä¸Šè·‘ã€‚å¦‚æœåœ¨å•GPUçš„æœºå™¨ä¸Šè·‘ï¼Œä¹Ÿèƒ½è·‘é€šï¼Œä½†æ˜¯å®é™…ä¸Šä½¿ç”¨çš„æ˜¯å•ä¸ªGPUã€‚


**1ï¼Œå‡†å¤‡æ•°æ®**

```python
import torch 
from torch import nn 

import torchvision 
from torchvision import transforms

import torchkeras 
```

```python
transform = transforms.Compose([transforms.ToTensor()])

ds_train = torchvision.datasets.MNIST(root="./data/minist/",train=True,download=True,transform=transform)
ds_valid = torchvision.datasets.MNIST(root="./data/minist/",train=False,download=True,transform=transform)

dl_train =  torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=4)
dl_valid =  torch.utils.data.DataLoader(ds_valid, batch_size=128, shuffle=False, num_workers=4)

print(len(ds_train))
print(len(ds_valid))
```

**2ï¼Œå®šä¹‰æ¨¡å‹**

```python
class CnnModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,10)]
        )
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)  
        return x

torchkeras.Model(CnnModule()).summary(input_shape=(1,32,32))

net = nn.DataParallel(CnnModule(), device_ids=[5,6])  #Attention this line!!!
model = torchkeras.Model(net)
```

**3ï¼Œè®­ç»ƒæ¨¡å‹**

```python
from sklearn.metrics import accuracy_score

def accuracy(y_pred,y_true):
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data
    return accuracy_score(y_true.cpu().numpy(),y_pred_cls.cpu().numpy()) 
    # æ³¨æ„æ­¤å¤„è¦å°†æ•°æ®å…ˆç§»åŠ¨åˆ°cpuä¸Šï¼Œç„¶åæ‰èƒ½è½¬æ¢æˆnumpyæ•°ç»„

device = torch.device("cuda:5" if torch.cuda.is_available() else "cpu")
model.compile(loss_func = nn.CrossEntropyLoss(),
             optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),
             metrics_dict={"accuracy":accuracy},device = device) # æ³¨æ„æ­¤å¤„compileæ—¶æŒ‡å®šäº†device

dfhistory = model.fit(3,dl_train = dl_train, dl_val=dl_valid, log_step_freq=100) 
```

```
Start Training ...

================================================================================2020-06-27 00:24:29
{'step': 100, 'loss': 1.063, 'accuracy': 0.619}
{'step': 200, 'loss': 0.681, 'accuracy': 0.764}
{'step': 300, 'loss': 0.534, 'accuracy': 0.818}
{'step': 400, 'loss': 0.458, 'accuracy': 0.847}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.412 |  0.863   |  0.128   |    0.961     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-27 00:24:35
{'step': 100, 'loss': 0.147, 'accuracy': 0.956}
{'step': 200, 'loss': 0.156, 'accuracy': 0.954}
{'step': 300, 'loss': 0.156, 'accuracy': 0.954}
{'step': 400, 'loss': 0.157, 'accuracy': 0.955}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   2   | 0.153 |  0.956   |  0.085   |    0.976     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-27 00:24:42
{'step': 100, 'loss': 0.126, 'accuracy': 0.965}
{'step': 200, 'loss': 0.147, 'accuracy': 0.96}
{'step': 300, 'loss': 0.153, 'accuracy': 0.959}
{'step': 400, 'loss': 0.147, 'accuracy': 0.96}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   3   | 0.146 |   0.96   |  0.119   |    0.968     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-27 00:24:48
Finished Training...
```


**4ï¼Œè¯„ä¼°æ¨¡å‹**

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory, "loss")
```

```python
plot_metric(dfhistory,"accuracy")
```

```python
model.evaluate(dl_valid)
```

**5ï¼Œä½¿ç”¨æ¨¡å‹**

```python
model.predict(dl_valid)[0:10]
```

**6ï¼Œä¿å­˜æ¨¡å‹**

```python
# save the model parameters
torch.save(model.net.module.state_dict(), "model_parameter.pkl")

net_clone = CnnModel()
net_clone.load_state_dict(torch.load("model_parameter.pkl"))

model_clone = torchkeras.Model(net_clone)
model_clone.compile(loss_func = nn.CrossEntropyLoss(),
             optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),
             metrics_dict={"accuracy":accuracy},device = device)
model_clone.evaluate(dl_valid)
```

### äº”ï¼Œtorchkeras.LightModelä½¿ç”¨GPU/TPUèŒƒä¾‹



ä½¿ç”¨torchkeras.LightModelå¯ä»¥éå¸¸å®¹æ˜“åœ°å°†è®­ç»ƒæ¨¡å¼ä»cpuåˆ‡æ¢åˆ°å•ä¸ªgpuï¼Œå¤šä¸ªgpuä¹ƒè‡³å¤šä¸ªtpu.



**1ï¼Œå‡†å¤‡æ•°æ®**

```python
import torch 
from torch import nn 

import torchvision 
from torchvision import transforms

import torchkeras 
```

```python
transform = transforms.Compose([transforms.ToTensor()])

ds_train = torchvision.datasets.MNIST(root="./data/minist/",train=True,download=True,transform=transform)
ds_valid = torchvision.datasets.MNIST(root="./data/minist/",train=False,download=True,transform=transform)

dl_train =  torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=4)
dl_valid =  torch.utils.data.DataLoader(ds_valid, batch_size=128, shuffle=False, num_workers=4)

print(len(ds_train))
print(len(ds_valid))
```

**2ï¼Œå®šä¹‰æ¨¡å‹**

```python
import torchkeras 
import pytorch_lightning as pl 

class CnnNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,10)]
        )
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)
        return x
    

class Model(torchkeras.LightModel):
    
    #loss,and optional metrics
    def shared_step(self,batch)->dict:
        x, y = batch
        prediction = self(x)
        loss = nn.CrossEntropyLoss()(prediction,y)
        preds = torch.argmax(nn.Softmax(dim=1)(prediction),dim=1).data
        acc = torch.mean(1-torch.abs(y-preds).float())
#         acc = pl.metrics.functional.accuracy(preds, y)
        dic = {"loss":loss,"acc":acc} 
        return dic
    
    #optimizer,and optional lr_scheduler
    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.0001)
        return {"optimizer":optimizer,"lr_scheduler":lr_scheduler}
    
pl.seed_everything(1234)
net = CnnNet()
model = Model(net)

torchkeras.summary(model,input_shape=(1,32,32))
print(model)
```

**3ï¼Œè®­ç»ƒæ¨¡å‹**

```python
ckpt_cb = pl.callbacks.ModelCheckpoint(monitor='val_loss')

# set gpus=0 will use cpuï¼Œ
# set gpus=1 will use 1 gpu
# set gpus=2 will use 2gpus 
# set gpus = -1 will use all gpus 
# you can also set gpus = [0,1] to use the  given gpus
# you can even set tpu_cores=2 to use two tpus 

trainer = pl.Trainer(max_epochs=10,gpus = [2,3], accelerator="dp", callbacks=[ckpt_cb]) 

trainer.fit(model,dl_train,dl_valid)
```

**4ï¼Œè¯„ä¼°æ¨¡å‹**

```python
import pandas as pd 

history = model.history
dfhistory = pd.DataFrame(history) 
dfhistory 
```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory,"loss")
```

```python
plot_metric(dfhistory,"acc")
```

```python
results = trainer.test(model, dataloaders=dl_valid, verbose = False)
print(results[0])
```

**5ï¼Œä½¿ç”¨æ¨¡å‹**

```python
def predict(model,dl):
    model.eval()
    preds = torch.cat([model.forward(t[0].to(model.device)) for t in dl])
    
    result = torch.argmax(nn.Softmax(dim=1)(preds),dim=1).data
    return(result.data)

result = predict(model,dl_valid)
result 
```

**6ï¼Œä¿å­˜æ¨¡å‹**

```python
print(ckpt_cb.best_model_score)
model.load_from_checkpoint(ckpt_cb.best_model_path)

best_net  = model.net
torch.save(best_net.state_dict(),"./data/net.pt")
```

```python
net_clone = CnnNet()
net_clone.load_state_dict(torch.load("./data/net.pt"))
model_clone = Model(net_clone)
trainer = pl.Trainer(gpus=[5,6],accelerator="dp")
result = trainer.test(model_clone,dataloaders=dl_valid, verbose = False) 

print(result)
```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![ç®—æ³•ç¾é£Ÿå±‹logo.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)
